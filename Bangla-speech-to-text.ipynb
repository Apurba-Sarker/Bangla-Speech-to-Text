{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":37174,"databundleVersionId":3938797,"sourceType":"competition"},{"sourceId":3886074,"sourceType":"datasetVersion","datasetId":2308987},{"sourceId":3889637,"sourceType":"datasetVersion","datasetId":2311133},{"sourceId":3911403,"sourceType":"datasetVersion","datasetId":2323175}],"dockerImageVersionId":30204,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install jiwer\n!pip install bnunicodenormalizer\n!pip install image-classifiers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom IPython import display\nfrom jiwer import wer\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nimport tensorflow as tf\n\nimport warnings\nimport random\nimport matplotlib.pyplot as plt\nimport json\n\nfrom tqdm.auto import tqdm\nfrom pandarallel import pandarallel\nfrom IPython.display import display,Audio\nfrom bnunicodenormalizer import Normalizer \nfrom pprint import pprint\nfrom multiprocessing import Process\nfrom classification_models.tfkeras import Classifiers\n\npandarallel.initialize(progress_bar=True,nb_workers=8)\ntqdm.pandas()\nwarnings.filterwarnings('ignore')\nbnorm=Normalizer()\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nprint(gpus)\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\nstrategy = tf.distribute.get_strategy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"errors=[\"common_voice_bn_31727562\",\n        'common_voice_bn_30998934',\n        'common_voice_bn_31595526',\n        'common_voice_bn_31534853',\n        'common_voice_bn_31518061',\n        'common_voice_bn_31518373',\n        'common_voice_bn_31613621',\n        'common_voice_bn_31555333',\n        'common_voice_bn_31772113',\n        'common_voice_bn_31605391',\n        'common_voice_bn_31631175',\n        'common_voice_bn_31563901',\n        'common_voice_bn_31691690',\n        'common_voice_bn_31692010',\n        'common_voice_bn_31683653',\n        'common_voice_bn_31692182',\n        'common_voice_bn_31519976',\n        'common_voice_bn_31675793',\n        'common_voice_bn_31019914',\n        'common_voice_bn_31660287',\n        'common_voice_bn_31660384',\n        'common_voice_bn_31557261',\n        'common_voice_bn_31633101',\n        'common_voice_bn_31599243',\n        'common_voice_bn_31521515',\n        'common_voice_bn_31777802',\n        'common_voice_bn_31777848',\n        'common_voice_bn_31669646',\n        'common_voice_bn_31566083',\n        'common_voice_bn_31530331',\n        'common_voice_bn_31727697',\n        'common_voice_bn_31513270',\n        'common_voice_bn_31686295',\n        'common_voice_bn_31753693',\n        'common_voice_bn_31686334',\n        'common_voice_bn_31765546',\n        'common_voice_bn_31765548',\n        'common_voice_bn_31662742',\n        'common_voice_bn_31704856',\n        'common_voice_bn_31635344',\n        'common_voice_bn_31618327',\n        'common_voice_bn_31743074',\n        'common_voice_bn_31678862',\n        'common_voice_bn_31626674',\n        'common_voice_bn_31626677',\n        'common_voice_bn_31523889',\n        'common_voice_bn_31610804',\n        'common_voice_bn_31769538',\n        'common_voice_bn_31533273',\n        'common_voice_bn_31445621',\n        'common_voice_bn_31620650']\n#---------------\n# data filtering\n#---------------\ndef filter_votes(x):\n    p=x[\"path\"]\n    # avoid error data\n    for pe in errors:\n        if pe in p:\n            return None\n    # now process votes\n    up=x[\"up_votes\"]\n    down=x[\"down_votes\"]\n    if up-down<=0:\n        return \"unv\"\n    elif up==0:\n        return \"unv\"\n    else:\n        return up\n# ------------------------- train data----------------------------------------\ntrain_path=\"../input/train-wavs-voted-dl-sprint/train_files_wav\"\ndf=pd.read_csv(\"../input/dlsprint/train.csv\")\ndf[\"path\"]=df[\"path\"].progress_apply(lambda x:os.path.join(train_path,x).replace(\".mp3\",\".wav\"))\nprint(\"Total Data before filtering:\",len(df))\ndf[\"up_votes\"]=df.progress_apply(lambda x:filter_votes(x),axis=1)\ndf.dropna(subset=[\"up_votes\"],inplace=True)\ntrain=df.loc[df.up_votes!=\"unv\"]\ntrain.reset_index(drop=True,inplace=True)\nprint(\"Total Data after filtering:\",len(train))\nunv_df=df.loc[df.up_votes==\"unv\"]\nunv_df.reset_index(drop=True,inplace=True)\nprint(\"Total unverified Data after filtering:\",len(unv_df))\ntrain=train[[\"path\",\"sentence\"]]\nunv_df=unv_df[[\"path\",\"sentence\"]]\n# ------------------------- eval data----------------------------------------\nval_path=\"../input/validation-fileswav-format/validation_files_wav\"\nvalid=pd.read_csv(\"../input/dlsprint/validation.csv\")\nvalid=valid[[\"path\",\"sentence\"]]\nvalid[\"path\"]=valid[\"path\"].progress_apply(lambda x:os.path.join(val_path,x).replace(\".mp3\",\".wav\"))\nprint(\"Total validation Data :\",len(valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"characters = [x for x in ['।', 'ঁ', 'ং', 'ঃ', 'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঋ', 'এ', 'ঐ', 'ও', 'ক', 'খ', 'গ', 'ঘ', 'ঙ', 'চ', 'ছ', 'জ', 'ঝ', 'ঞ', 'ট', 'ঠ', 'ড', 'ঢ', 'ণ', 'ত', 'থ', 'দ', 'ধ', 'ন', 'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', 'ল', 'শ', 'ষ', 'স', 'হ', '়', 'া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', 'ো', 'ৌ', '্', 'ৎ', 'ড়', 'য়', '০', '১', '২', '৩', '৪', '৫', '৬', '৭', '৮', '৯',' ']]\n# Mapping characters to integers\nchar_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n# Mapping integers back to original characters\nnum_to_char = keras.layers.StringLookup(\n    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n)\n\nprint(\n    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n    f\"(size ={char_to_num.vocabulary_size()})\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# An integer scalar Tensor. The window length in samples.\nframe_length = 200\n# An integer scalar Tensor. The number of samples to step.\nframe_step = 80\n# An integer scalar Tensor. The size of the FFT to apply.\n# If not provided, uses the smallest power of 2 enclosing frame_length.\nfft_length = 256\n\n\ndef encode_single_sample(wav_file, label):\n    ###########################################\n    ##  Process the Audio\n    ##########################################\n    # 1. Read wav file\n    file = tf.io.read_file(wavs_path + wav_file)\n    # 2. Decode the wav file\n    audio, _ = tf.audio.decode_wav(file)\n    audio = tf.squeeze(audio, axis=-1)\n    # 3. Change type to float\n    audio = tf.cast(audio, tf.float32)\n    # 4. Get the spectrogram\n    spectrogram = tf.signal.stft(\n        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n    )\n    # 5. We only need the magnitude, which can be derived by applying tf.abs\n    spectrogram = tf.abs(spectrogram)\n    spectrogram = tf.math.pow(spectrogram, 0.5)\n    # 6. normalisation\n    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n    ###########################################\n    ##  Process the label\n    ##########################################\n    # 7. Convert label to Lower case\n    #label = tf.strings.lower(label)\n    # 8. Split the label\n    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n    # 9. Map the characters in label to numbers\n    label = char_to_num(label)\n    # 10. Return a dict as our model is expecting two inputs\n    return spectrogram,label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfilenames = os.listdir(\"../input/train-wavs-voted-dl-sprint/train_files_wav\")\nfrom tqdm import tqdm\n\ndf = train.iloc[:len(filenames)]\nto_take = []\nlabel = []\n\nfor i in tqdm(range(len(train))):\n    if train.path.iloc[i] in filenames:\n        to_take.append(train.path.iloc[i])\n        label.append(train.sentence.iloc[i])\ndf.path = to_take\ndf.sentence = label\n\ntrain = df\nprint(train.shape)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wavs_path = \"../input/train-wavs-voted-dl-sprint/train_files_wav/\"\n\nbatch_size = 32\n# Define the trainig dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n    (list(train[\"path\"]), list(train[\"sentence\"]))\n)\ntrain_dataset = (\n    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n    .padded_batch(batch_size)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\n\nwavs_path = \"../input/validation-fileswav-format/validation_files_wav/\"\n# Define the validation dataset\nvalidation_dataset = tf.data.Dataset.from_tensor_slices(\n    (list(valid[\"path\"]), list(valid[\"sentence\"]))\n)\nvalidation_dataset = (\n    validation_dataset.map((encode_single_sample), num_parallel_calls=tf.data.AUTOTUNE)\n    .padded_batch(batch_size)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wavs_path = \"../input/train-wavs-voted-dl-sprint/train_files_wav/\"\n\nfig = plt.figure(figsize=(8, 5))\nfor batch in train_dataset.take(1):\n    spectrogram = batch[0][0].numpy()\n    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n    label = batch[1][0]\n    # Spectrogram\n    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n    print(label)\n    ax = plt.subplot(2, 1, 1)\n    ax.imshow(spectrogram, vmax=1)\n    ax.set_title(label)\n    ax.axis(\"off\")\n    # Wav\n    file = tf.io.read_file(wavs_path + list(train[\"path\"])[0])\n    audio, _ = tf.audio.decode_wav(file)\n    print(_)\n    audio = audio.numpy()\n    ax = plt.subplot(2, 1, 2)\n    plt.plot(audio)\n    ax.set_title(\"Signal Wave\")\n    ax.set_xlim(0, len(audio))\n    display.display(display.Audio(np.transpose(audio), rate=16000))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CTCLoss(y_true, y_pred):\n    # Compute the training-time loss value\n    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n    print(input_length,label_length)\n    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n    return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\n\nclass DotAttention(tf.keras.layers.Layer):\n   \n    def __init__(self):\n        super().__init__()\n        self.inf_val=-1e9\n        \n    def call(self,q, k, v, mask):\n        \n        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n       \n        # scale matmul_qk\n        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n        # add the mask to the scaled tensor.\n        if mask is not None:\n            scaled_attention_logits += (mask * self.inf_val)\n\n        # softmax is normalized on the last axis (seq_len_k) so that the scores\n        # add up to 1.\n        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n        return output\n    \n#--------------------------------------------------------------------\n\nclass PositionalEncoding(tf.keras.layers.Layer):\n    '''\n    tensorflow wrapper for positional encoding layer\n    args:\n      num_seq  :   incoming sequence length\n      projection_dim  :   projection_dim\n      use_torch_weights : torch weights help converge faster for basic dot attention\n    '''\n    def __init__(self,num_seq,projection_dim,use_torch_weights=False):\n        super(PositionalEncoding, self).__init__()\n        self.use_torch_weights=use_torch_weights\n        self.projection_dim=projection_dim\n        self.num_seq = num_seq\n        self.projection = tf.keras.layers.Dense(units=projection_dim)\n        if use_torch_weights:\n            pos_emb              = nn.Embedding(num_seq+1,projection_dim)\n            pos_emb_weight       = pos_emb.weight.data.numpy()\n            self.position_embedding = tf.keras.layers.Embedding(input_dim=num_seq+1, output_dim=projection_dim,weights=[pos_emb_weight])\n        else:\n            \n            self.position_embedding = tf.keras.layers.Embedding(input_dim=num_seq, output_dim=projection_dim)\n\n    def call(self, x):\n        positions = tf.range(start=0, limit=self.num_seq, delta=1)\n        if x is None:\n            return self.position_embedding(positions)\n        encoded = self.projection(x) + self.position_embedding(positions)\n        return encoded\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({'num_seq': self.num_seq,'projection_dim':self.projection_dim,\"use_torch_weights\":use_torch_weights})\n        return config\n\n#--------------------------------------------------------------------\n \nclass TransformerBlock(tf.keras.layers.Layer):\n    '''\n        transformer encoder block based on multihead self-attention\n    '''\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.embed_dim=embed_dim\n        self.num_heads=num_heads\n        self.ff_dim   =ff_dim\n\n        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tf.keras.Sequential(\n            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({'embed_dim': self.embed_dim,\n                       'num_heads': self.num_heads,\n                       'ff_dim':self.ff_dim})\n        return config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def attend(x,num_heads,num_blocks,reshape=True):\n    '''\n        basic self-attention wrapper\n        args:\n            x : input tensor\n            num_heads : heads to use in multihead attention\n            num_blocks: how many attention blocks to use \n    '''\n    bs,h,w,nc=x.shape\n    x = tf.keras.layers.Reshape((h*w,nc))(x)\n    x = PositionalEncoding(h*w,nc)(x)\n    for _ in range(num_blocks):\n        x=TransformerBlock(embed_dim=nc, num_heads=num_heads,ff_dim=4*nc)(x)\n    if reshape:\n        x = tf.keras.layers.Reshape((h,w,nc))(x)\n    return x\n\n\ndef create_basic_model(cfg):\n    '''\n        creates a basic cnn-seftattention-positional-attention based model\n        **flow**\n        cnn_feat(input_image)---> h//f,w//f,c shaped tensor = feat\n        attend(feat)---> self attention applied on the features= enc\n        pos_attention(enc)--> align encoded features with positional data=logits\n    '''\n    #-----------cnn feature extractor------------------\n    cnn,_ = Classifiers.get(cfg.backbone)\n    cnn = cnn(cfg.img_dim,weights=None,include_top=False)\n    inp = cnn.input\n    x   = cnn.output\n    bs,h,w,fc=x.shape\n    if fc!=cfg.embed_dim:\n        x=tf.keras.layers.Conv2D(cfg.embed_dim,3,padding='same')(x)\n    print(\"model input:\",inp.shape)\n    print(\"cnn feat:\",x.shape)\n    #-----------self attention------------------\n    x=attend(x,cfg.num_blocks,cfg.num_heads,reshape=False)\n    print(\"feat attention (seq,embed_dim):\",x.shape)\n    #-----------positional attention------------------\n    pos=PositionalEncoding(cfg.pos_max,cfg.embed_dim,use_torch_weights=True)(None)\n    attn=DotAttention()(pos,x,x,None)\n    print(\"positional attention (pos_max,embed_dim):\",attn.shape)\n    x=tf.keras.layers.Dense(cfg.logits_len)(attn)\n    print(\"logits(pos_max,logits_len):\",x.shape)\n    model = tf.keras.Model(inputs=inp,outputs=x)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class cfg:\n    backbone='resnet18'\n    img_dim =(64,1088,1)     #features.shape\n    pos_max =200             #np.array(encode_label(sen)).shape\n    num_heads=8              # number of self-attention heads\n    num_blocks=4             # number of transformer blocks\n    embed_dim =256           # reduced channel for sequencing\n    logits_len =85   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model=create_basic_model(cfg)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------metrics---------------------------\ndef C_acc(y_true, y_pred):\n    '''\n        calculates how many charecters are predicted correctly\n    '''\n    pad_value=0 # label pad index\n    accuracies = tf.equal(tf.cast(y_true,tf.int64), tf.argmax(y_pred, axis=2))\n    mask = tf.math.logical_not(tf.math.equal(y_true,pad_value))\n    accuracies = tf.math.logical_and(mask, accuracies)\n    accuracies = tf.cast(accuracies, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n#------------------loss--------------------------\nclass CharLoss(tf.keras.losses.Loss):\n    \"\"\"\n        a loss function to estimate charecter accuracy loss ignoring pad value\n    \"\"\"\n    def __init__(self,pad_value):\n        super(CharLoss, self).__init__(name=\"char_loss\")\n        self.pad_value=pad_value\n        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n    def call(self, y_true, y_pred):\n        mask = tf.math.logical_not(tf.math.equal(y_true, self.pad_value))\n        loss_ = self.loss_object(y_true, y_pred)\n        mask = tf.cast(mask, dtype=loss_.dtype)\n        loss_ *= mask\n        return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n\nclass CTCLoss(tf.keras.losses.Loss):\n    \"\"\" A class that wraps the function of tf.nn.ctc_loss. \n    \n    Attributes:\n        logits_time_major: If False (default) , shape is [batch, time, logits], \n            If True, logits is shaped [time, batch, logits]. \n        blank_index: Set the class index to use for the blank label. default is\n            -1 (num_classes - 1). \n    \"\"\"\n\n    def __init__(self, logits_time_major=False, name='ctc_loss'):\n        super().__init__(name=name)\n        self.logits_time_major = logits_time_major\n\n    def call(self, y_true, y_pred):\n        \"\"\" \n            Computes CTC (Connectionist Temporal Classification) loss. \n        \"\"\"\n        y_true = tf.cast(y_true, tf.int32)\n        logit_length = tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1])\n        label_length = tf.fill([tf.shape(y_true)[0]], tf.shape(y_true)[1])\n        loss = tf.nn.ctc_loss(\n            labels=y_true,\n            logits=y_pred,\n            label_length=label_length,\n            logit_length=logit_length,\n            logits_time_major=self.logits_time_major,\n            blank_index=0)\n        return tf.math.reduce_mean(loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# early stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=5, \n                                                  verbose=1, \n                                                  mode = 'auto') \ncallbacks = [tf.keras.callbacks.ModelCheckpoint(\"model.h5\",\n                                                save_best_only=True,\n                                                save_weights_only=True,\n                                                verbose=1),\n             early_stopping]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n\n    lr_schedule = tf.keras.experimental.CosineDecay(initial_learning_rate=0.0001,\n                                                             decay_steps=600000,\n                                                             alpha= 0.01)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr_schedule),\n                  loss=CharLoss(char_to_num.get_vocabulary()),\n                  metrics=[C_acc])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A utility function to decode the output of the network\ndef decode_batch_predictions(pred):\n    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n    # Use greedy search. For complex tasks, you can use beam search\n    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n    # Iterate over the results and get back the text\n    output_text = []\n    for result in results:\n        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n        output_text.append(result)\n    return output_text\n\n\n# A callback class to output a few transcriptions during training\nclass CallbackEval(keras.callbacks.Callback):\n    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n\n    def __init__(self, dataset):\n        super().__init__()\n        self.dataset = dataset\n\n    def on_epoch_end(self, epoch: int, logs=None):\n        predictions = []\n        targets = []\n        for batch in self.dataset:\n            X, y = batch\n            batch_predictions = model.predict(X)\n            batch_predictions = decode_batch_predictions(batch_predictions)\n            predictions.extend(batch_predictions)\n            for label in y:\n                label = (\n                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n                )\n                targets.append(label)\n        wer_score = wer(targets, predictions)\n        print(\"-\" * 100)\n        print(f\"Word Error Rate: {wer_score:.4f}\")\n        print(\"-\" * 100)\n        for i in np.random.randint(0, len(predictions), 2):\n            print(f\"Target    : {targets[i]}\")\n            print(f\"Prediction: {predictions[i]}\")\n            print(\"-\" * 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS=1\nBATCH_SIZE = 8\nSTEPS_PER_EPOCH=len(train_dataset)//BATCH_SIZE\nEVAL_STEPS=len(validation_dataset)//BATCH_SIZE\n\n\nhistory=model.fit(train_dataset,\n                  epochs=EPOCHS,\n                  steps_per_epoch=STEPS_PER_EPOCH,\n                  verbose=1,\n                  validation_data=validation_dataset,\n                  validation_steps=EVAL_STEPS, \n                  callbacks=callbacks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\ntargets = []\ni=0\nfor batch in tqdm(validation_dataset):\n    X, y = batch\n    batch_predictions = model.predict(X)\n    batch_predictions = decode_batch_predictions(batch_predictions)\n    print(batch_predictions)\n    i+=1\n    if i>5:\n        break\n    predictions.extend(batch_predictions)\n    for label in y:\n        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n        targets.append(label)\nwer_score = wer(targets, predictions)\nprint(\"-\" * 100)\nprint(f\"Word Error Rate: {wer_score:.4f}\")\nprint(\"-\" * 100)\nfor i in np.random.randint(0, len(predictions), 5):\n    print(f\"Target    : {targets[i]}\")\n    print(f\"Prediction: {predictions[i]}\")\n    print(\"-\" * 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/dlsprint/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.path = test.path.apply(lambda x:x.replace(\".mp3\",\".wav\"))\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wavs_path = \"../input/test-wav-files-dl-sprint/test_files_wav/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = tf.data.Dataset.from_tensor_slices(\n    (list(test[\"path\"]), list(test[\"sentence\"]))\n)\ntest_dataset = (\n    test_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n    .padded_batch(batch_size)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\ntargets = []\nfor batch in tqdm(validation_dataset):\n    X, y = batch\n    batch_predictions = model.predict(X)\n    batch_predictions = decode_batch_predictions(batch_predictions)\n    predictions.extend(batch_predictions)\n    for label in y:\n        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n        targets.append(label)\nwer_score = wer(targets, predictions)\nprint(\"-\" * 100)\nprint(f\"Word Error Rate: {wer_score:.4f}\")\nprint(\"-\" * 100)\nfor i in np.random.randint(0, len(predictions), 5):\n    print(f\"Target    : {targets[i]}\")\n    print(f\"Prediction: {predictions[i]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(\"../input/dlsprint/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.sentence = predictions\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission_ctc_tutorial\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}